name: DataOps

on:
  schedule:
    - cron: '0 2 * * *'

jobs:
  run:

    name: Hydat-Forge Data Pipeline
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: conda-incubator/setup-miniconda@v2
        with:
          activate-environment: era5-forge
          environment-file: .binder/environment.yml
          python-version: 3.8
          auto-activate-base: false
      - shell: bash -l {0}
        run: |
            sudo apt install awscli
            mkdir -p ~/.aws
            touch ~/.aws/credentials
            echo "
            [default]
            aws_access_key_id = ${{ secrets.AWS_ACCESS_KEY_ID }}
            aws_secret_access_key = ${{ secrets.AWS_SECRET_KEY_ID }}
            region = us-east-1
            source_profile = default" > ~/.aws/credentials
            aws configure set s3.max_concurrent_requests 100 --profile default

            touch ~/.cdsapirc
            echo "
            url: https://cds.climate.copernicus.eu/api/v2
            key: ${{ secrets.CDS_API_KEY }}
            " > ~/.cdsapirc

            python pipeline.py
